---
title: "Factors leading to an Approved TSA Claim"
author: "Samba Diallo, David Jia, Raquel Figueroa"
date: "5/13/2018"
output: html_document
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(prompt=TRUE, comment="", echo=TRUE)
library(rpart)
library(rpart.plot)
library(e1071)
dat = read.csv("https://raw.githubusercontent.com/RaquelFigueroa/tsa_claims/master/tsa_claims.csv")
source("https://raw.githubusercontent.com/grbruns/cst383/master/lin-regr-util.R")
set.seed(123)
```

## Project Introduction

For this project, we are investigating property and injury claims filed with the Terminal Security Agency (TSA). We hope to determine patterns and correlations between claims made at airports across the United States...[TODO]


## Project Data

The TSA claims dataset we are using is composed of 204,267 rows and 13 columns. There are many empty values so it will be assumed that many of the rows will be discarded depending on what sorts of features will be used for model creation and data exploration.

This dataset may be found at the link below:
https://www.kaggle.com/terminal-security-agency/tsa-claims-database/data


## Initial data exploration
It should be noted that running the `complete.cases()` function and counting the total number of complete rows showed that all rows are complete. This means that there are no `NA` values, but through the `summary()` function we can see that there are other values to be aware of. There are a notable number of empty string values, dashes ('-'), and the value `Other`. To avoid unintentionally skewing results, these values will need to be discarded or imputed.

***

Another helpful starting function is `str()`.
Through this function, we can see that all features are factors.

We can also determine that there are 204267 rows of data and 13 features.
One interesting feature is `Claim.Number` as it would be assumed that there should be a unique number for each of the unique rows of data. There are 23 less unique claim numbers than there are rows. This could mean that some of the claims needed to be resubmitted and this should be investigated further to ensure some data is not counted more than once.

The data features are as follows:

* Continuous Features
    + Claim.Amount
    + Close.Amount
    
* Categorical Features
    + Claim.Number
    + Date.Received
    + Incident.Date
    + Airport.Code
    + Airport.Name
    + Airline.Name
    + Claim.Type
    + Claim.Site
    + Item
    + Status
    + Disposition

***

## Data Cleaning and Preprocessing

While exploring the data, we determined that many of the empty string values or dashes will either need to be deleted or imputed with values from other columns of the same row. For example, if a row had an empty `Status` value but a `Disposition` value of "Denied" then the value of `Status` was set to equal that of `Disposition`. If both `Status` and Disposition` were empty, then a final action could not be confidently determined and those rows were deleted from the dataset.

The dollar amounts for `Claim.Amount` and `Close.Amount` used characters that made them unusable. The dollar amounts were processed to look like double values so they may be visualizaed later in the project.

There are some rows in the dataset that did not contain data but information about the dataset instead. These rows were deleted to prevent any future errors when exploring the data.

There were also some very large `Claim.Amount` values that belonged to claimes that were denied. Any `Claim.Amount` value greater than `150000.00` was deleted from the data set.

After the dataset is cleaned and preprocessed, `86,285` rows remain.
```{r, echo=TRUE}
# Delete rows with anomalous formats
glitchRows = c(186743,97232,145145,186744,195600)
dat = dat[-glitchRows,]

# Delete rows where Airport.Code and Airport.Name have values "" or "-":
keepRows = dat$Airport.Code != "" & dat$Airport.Code != "-" & dat$Airport.Name != "" & dat$Airport.Name != "-"
dat = dat[keepRows,]

# Delete rows where Airline.Name  has value of "" or "-":
keepRows = dat$Airline.Name != "" & dat$Airline.Name != "-"
dat = dat[keepRows,]

# Delete rows where Claim.Type  has value of "" or "-":
keepRows = dat$Claim.Type != "" & dat$Claim.Type != "-"
dat = dat[keepRows,]

# Delete rows where Claim.Site  has value of "", "-", or "Other:
keepRows = dat$Claim.Site != "" & dat$Claim.Site != "-" & dat$Claim.Site != "Other"
dat = dat[keepRows,]

# Delete rows where Item  has value of "", "-", or "Other:
keepRows = dat$Item != "" & dat$Item != "-" & dat$Item != "Other"
dat = dat[keepRows,]

# Make sure all similar variants of Status values are consolidated and delete values not applicable
keepRows = dat$Status != "In litigation" & dat$Status != "In review" & dat$Status != "Claim has been assigned for further investigation"  & dat$Status != "Claim entered" & dat$Status != "Pending response from claimant" & dat$Status !=  "Closed as a contractor claim" & dat$Status != "Canceled"
dat = dat[keepRows,]
levels(dat$Status)[levels(dat$Status) %in% c('Settled','Settle')] <- 'Settled'
levels(dat$Status)[levels(dat$Status) %in% c('Denied','Deny')] <- 'Denied'

# Make sure all similar variants of Disposition values are consolidated
levels(dat$Disposition)[levels(dat$Disposition) %in% c('Settled','Settle')] <- 'Settled'
levels(dat$Disposition)[levels(dat$Disposition) %in%  c('Denied','Deny')] <- 'Denied'


# If Disposition is "Approve in Full" and Claim.Amount or Close.Amount is not, set the empty amount value to the other or delete row if both empty
new.claim = apply(dat, 1, function(x){
    if (x["Disposition"] == "Approve in Full") {
      if (x["Claim.Amount"] == "" & x["Close.Amount"] != "") {
        x["Claim.Amount"] = x["Close.Amount"]
      }
    }
  x["Claim.Amount"]
})

new.close = apply(dat, 1, function(x){
    if (x["Disposition"] == "Approve in Full") {
      if (x["Claim.Amount"] != "" & x["Close.Amount"] == ""){
        x["Close.Amount"] = x["Claim.Amount"]
      }
    }
  x["Close.Amount"]
})

dat$Claim.Amount = new.claim
dat$Close.Amount = new.close

keepRows = dat$Claim.Amount != "" | dat$Close.Amount != "" | dat$Disposition != "Approve in Full"
dat = dat[keepRows,]

# Delete rows where Status  has value of "-"
keepRows = dat$Status != "-"
dat = dat[keepRows,]


# If Status is "Approve in Full" set it to equal "Approved" and set Close.Amount to equal Claim.Amount and Disposition to equal "Approved"
new.status = apply(dat, 1, function(x){
    if (x["Status"] == "Approve in Full") {
      x["Status"] = "Approved"
    }
  x["Status"]
})

new.disposition = apply(dat, 1, function(x){
    if (x["Status"] == "Approve in Full") {
      x["Disposition"] = "Approve in Full"
    }
  x["Disposition"]
})

new.close = apply(dat, 1, function(x){
    if (x["Status"] == "Approve in Full") {
      x["Close.Amount"] = x["Claim.Amount"]
    }
  x["Close.Amount"]
})

dat$Status = new.status 
dat$Disposition = new.disposition
dat$Close.Amount = new.close


# If Status is "Insufficient..." and Disposition is "Denied", set Status = "Denied" and Close.Amount == "$0.00"
new.status = apply(dat, 1, function(x){
    if (x["Status"] == "Insufficient; one of the following items required: sum certain; statement of fact; signature; location of incident; and date." & x["Disposition"] == "Denied") {
      x["Status"] = "Denied"
    }
  x["Status"]
})

new.close = apply(dat, 1, function(x){
    if (x["Status"] == "Insufficient; one of the following items required: sum certain; statement of fact; signature; location of incident; and date." & x["Disposition"] == "Denied") {
      x["Close.Amount"] = "$0.00"
    }
  x["Close.Amount"]
})

dat$Status = new.status 
dat$Close.Amount = new.close


# If Status is "Insufficient..." and Disposition is "Approve in Full", set Status = "Approved"
new.status = apply(dat, 1, function(x){
    if (x["Status"] == "Insufficient; one of the following items required: sum certain; statement of fact; signature; location of incident; and date." & x["Disposition"] == "Approve in Full") {
      x["Status"] = "Approved"
    }
  x["Status"]
})

dat$Status = new.status 
dat$Status = factor(dat$Status)


# If Status is "Insufficient..." delete rows
keepRows = dat$Status != "Insufficient; one of the following items required: sum certain; statement of fact; signature; location of incident; and date."
dat = dat[keepRows,]


# If Status is "Denied" set Close.Amount = "$0.00"
new.close = apply(dat, 1, function(x){
    if (x["Status"] == "Denied" & x["Close.Amount"] != "$0.00") {
      x["Close.Amount"] = "$0.00"
    }
    if (x["Close.Amount"] == "$0.00 "){
      x["Close.Amount"] == "$0.00"
    }
  x["Close.Amount"]
})

dat$Close.Amount = new.close

# If Status is "Denied" and Close.Amount = "$0.00", set Disposition = "Denied"
new.disposition = apply(dat, 1, function(x){
    if (x["Status"] == "Denied" & x["Close.Amount"] == "$0.00" & x["Disposition"] == "") {
      x["Disposition"] = "Denied"
    }
  x["Disposition"]
})
dat$Disposition = new.disposition

# If Status is Close.Amount = "$0.00 " and Disposition = "Approve in Full" set Close.Amount to equal Claim.Amount
new.close = apply(dat, 1, function(x){
    if (x["Close.Amount"] == "$0.00 " & x["Disposition"] == "Approve in Full") {
      x["Close.Amount"] = x["Claim.Amount"]
    }
  x["Close.Amount"]
})
dat$Close.Amount = new.close

# If Status is "Settled" and Disposition == "" and Close.Amount = "", delete row
keepRows = dat$Status != "Settled" | dat$Disposition != "" | dat$Close.Amount != ""
dat = dat[keepRows,]
dat$Status = factor(dat$Status)


# Delete any final rows with empty values
keepRows = dat$Claim.Amount != ""
dat = dat[keepRows,]

keepRows = dat$Incident.Date != ""
dat = dat[keepRows,]

keepRows = dat$Date.Received != ""
dat = dat[keepRows,]

# Delete unnecessary levels
dat$Disposition = factor(dat$Disposition)
dat$Claim.Number = factor(dat$Claim.Number)
dat$Date.Received = factor(dat$Date.Received)
dat$Incident.Date = factor(dat$Incident.Date)
dat$Airport.Code= factor(dat$Airport.Code)
dat$Airport.Name= factor(dat$Airport.Name)
dat$Airline.Name= factor(dat$Airline.Name)
dat$Claim.Type= factor(dat$Claim.Type)
dat$Claim.Site= factor(dat$Claim.Site)
dat$Item= factor(dat$Item)


# Format Claim.Amount and Close.Amount values
dat$Claim.Amount = gsub(";","",dat$Claim.Amount)
dat$Claim.Amount = gsub("[$]","",dat$Claim.Amount)
dat$Claim.Amount = as.numeric(gsub(" ","",dat$Claim.Amount))

dat$Close.Amount = gsub(";","",dat$Close.Amount)
dat$Close.Amount = gsub("[$]","",dat$Close.Amount)
dat$Close.Amount = as.numeric(gsub(" ","",dat$Close.Amount))

# Delete anomalous Claim.Amount values
keepRows = dat$Claim.Amount < 164500.00
dat = dat[keepRows,]

# Delete remaining rows where Claim.Amount is `0`
keepRows = dat$Claim.Amount > 0
dat = dat[keepRows,]

# Change Settle and Approve to Full to Approved for Predictions
dat$DeniedApproved = factor(ifelse(dat$Disposition == "Denied", "denied", "Approved"))
```

***

## Data exploration and visualization

Our investigation revolves around whether a claim filed with TSA is settled, approved in full, or denied. The initial data exploration looks into the proportion of claims that are denied or not.
Here we can see the majority of claims are denied and at a greater extent than claims being approved in full and settled combined. 
``` {r}
barplot(table(dat$Disposition), col = "red4")
```

***

### Claim Amount vs. Close Amount
When plotting the claim amounts and close amounts, we can see that there are some noticeable trends. Of course, there is a strong line along the x-axis at the y value of zero. This indicates the claim amounts that were denied, resulting in a close amount of zero. There is also a line along the diagonal of the plot, indicating claim amounts that were approved in the full amount, so the claim amount value is equal to the final close amount. 

Interestingly, there are some data points where the final close amount is greater than the filed claim amount. This occurence is seen in lower claim amount values and may be due to the fact that some claim amounts were rounded to the nearest dollar when the claims were closed.

``` {r}
plot(dat$Claim.Amount, dat$Close.Amount, xlim = c(0, quantile(dat$Claim.Amount, 0.995)), ylim = c(0, 10000), pch = ".", main = "Claim Amount Values vs. Close Amount", xlab = "Claim Amount Filed", ylab = "Final Close Amount", col = "navy")
```

***

### Percent Claim Amount Awarded

A `Percent.Closed` column is created to indicate the percentage of the claim amount awarded for each claim filed with the TSA. A zero amount indicates that the claim was denied and no amount of claim money was awarded. A `Percent.Closed` amount of `100` indicates that 100% of the claim amount requested was awarded, so the claim was `Approved in Full`.  

Graphing the density of `Percent.Closed` shows three distinct bumps: at `0`, `50`, and `100`. The highest bump is around the `0` mark, further enforcing that that the number of denied claims is larger then the numbers of claims that were approved-in-full and settled. The bump at the `50` mark shows that a fairly large amount of claims were settled at a value of about 50% of the initial claim amount filed. The last bump at the `100` mark indicates that many claims were approved-in-full and were closed for the full amount.

``` {r}
dat$Percent.Closed = 100 * dat$Close.Amount / dat$Claim.Amount
plot(density(100*dat$Close.Amount/dat$Claim.Amount), xlim = c(-10, 105), main = "Density Plot of Percent Claim Amount Awarded")
```

***
### Claim Types and Acceptance Rates

Now we explore different claim types and the percentage of the time they are accepted or settled. We use the table function in order to get the number of approved,settled, and denied claims based on claim type, then use prop.table in order to get the percentages of those counts by claim type.

``` {r}
# GET MOST OFTEN APPROVED AND SETTLED CLAIMS BAISED ON CLAIM TYPE

byType = prop.table(table(dat$Status,dat$Claim.Type),2)
byType = byType[,c(-1,-2,-3)]

par(mar=c(5,12,4,2))
barplot(byType, main="Car Distribution by Gears and VS",
  xlab="Percentage of Claims
  Green = Approved, Yellow = Settled, Red = Denied", col=c("green","red4","yellow"), beside=TRUE, horiz=TRUE, las=1)

```

I decided to exclude Complaint, Employee Loss, and Motor Vehicle claims from this graph because they dont relate to the type of claims that someone trying to scam the system would make. In this graph we see that many claims are denied. Some types of claims have a very low chance of approval, but it seems that a claim about property damage has a much higher chance of approval versus other types. In this and the passanger theft claim type cases, individually the approved and the settled percentages are less than denied, but if you consider that settling is still getting some reward, a fradululent claim has over 50% chance of getting a payout. 

***

In the previous section, we have discovered the percentage of times differnt claim types are accepted, settled, or denied. Now knowing those numbers, we can caluclate what would be the biggest bang for the buck.


```{r}
app = dat$Status == "Approved"
dapp = dat[app,c("Close.Amount","Claim.Type")]
numByType = as.data.frame.matrix(byType)
expectedReturn = apply(dat, 1, function(x){
  expect = 0
  if (x["Status"] == "Approved"){
    if(x["Claim.Type"] == "Passenger Property Loss") {
      expect = as.numeric(x["Close.Amount"]) * numByType[1,1]
    }
    else if(x["Claim.Type"] == "Passenger Theft") {
      expect = as.numeric(x["Close.Amount"]) * numByType[1,2]
    }
    else if(x["Claim.Type"] == "Personal Injury") {
      expect = as.numeric(x["Close.Amount"]) * numByType[1,3]
    }
    else if(x["Claim.Type"] == "Property Damage") {
      expect = as.numeric(x["Close.Amount"]) * numByType[1,4]
    }
  }
  else if (x["Status"] == "Settled"){
    if(x["Claim.Type"] == "Passenger Property Loss") {
      expect = as.numeric(x["Close.Amount"]) * numByType[3,1]
    }
    else if(x["Claim.Type"] == "Passenger Theft") {
      expect = as.numeric(x["Close.Amount"]) * numByType[3,2]
    }
    else if(x["Claim.Type"] == "Personal Injury") {
      expect = as.numeric(x["Close.Amount"]) * numByType[3,3]
    }
    else if(x["Claim.Type"] == "Property Damage") {
      expect = as.numeric(x["Close.Amount"]) * numByType[3,4]
    }
  }
  expect
})
dat$expectedReturn = expectedReturn

```

We took the percentage of times claims of that type were approved and settled and applied that to the close amount. This gives us a list of "Expected Returns" from the claims we have. Now below is a graph showing the expected returns from each of the claim types.

```{r}

app.set = dat$Status == "Approved" | dat$Status == "Settled"
ppl = mean(dat[ app.set & dat$Claim.Type == "Passenger Property Loss" , ]$expectedReturn)
pt = mean(dat[app.set & dat$Claim.Type == "Passenger Theft", ]$expectedReturn)
pd = mean(dat[app.set & dat$Claim.Type == "Property Damage" , ]$expectedReturn)
pi = mean(dat[app.set & dat$Claim.Type == "Personal Injury" , ]$expectedReturn)
vals = c(ppl,pt,pd,pi)
names(vals) = c("Passenger Property Loss","Passenger Theft","Property Damage","Personal Injury")
par(mar=c(5,10.5,4,2))
barplot(vals, main="Expected Return from Claim", xlab = "Dollars", col=c("blue","green","yellow","red4"), horiz=TRUE, las=1)



```

We can see here that the most lucrative of all the claims, based on the percentage of time that the claims get approved and settled and the mean amount they get approved and settled for, are personal injury claims. This makes sense, as people suing for injuries anywhere tend to sue for exorbitant amounts. The next highest, Passenger Theft, would make sense to be the second highest as people will claim higher amounts for goods stolen, even if the item was actually not worth that much. The second lowest amount for property damage makes sense as you cannot claim as much because the item is still there. If the whole item was gone like in theft or loss you could claim more. The lowest out of all of the types of claims are the passenger property loss ones. Claims for loss ussually are smaller than claims for theft, and the average close amount is also smaller. In addition to having small claims, they have the second highest denial rating under personal injury claims. This combination of the two factors results in an extremely low expected return. 


***
### Airport Locations

Another point of interest with the data is the locations of property damage and loss in the airport and their relation to the results of a claim filed. There are only three claims that took place outside of the checkpoint area and checked baggage. All three of those claims were reimbursed in full. The three claims are displayed in slightly larger dots in the graph below. The checked baggage and checkpoint claims seem evenly dispersed, but it could also be a useful predictor to test in a model. there may be some correlation between airport site and if a claim is denied, approved in full, or settled for a lesser amount.


The numbers of claims per airport location are listed in following table:

Airport Location | Number of Claims
------------------- | ---------------
Bus Station           | 1
Motor Vehicle        | 2 
Checkpoint        | 14754  
Checked Baggage    | 71528

``` {r}
plot(dat$Claim.Amount, dat$Close.Amount, xlim = c(0, quantile(dat$Claim.Amount, 0.925)), ylim = c(0, 1700), pch = ".", main = "Claim Amount Values vs. Close Amount by Airport Site", xlab = "Claim Amount Filed", ylab = "Final Close Amount", col = "navy")

points(Close.Amount ~ Claim.Amount, data = dat[dat$Claim.Site == "Checked Baggage",],pch = ".", col = "red4")

points(Close.Amount ~ Claim.Amount, data = dat[dat$Claim.Site == "Checkpoint",], pch = ".",col = "mediumblue")

points(Close.Amount ~ Claim.Amount, data = dat[dat$Claim.Site == "Motor Vehicle",], pch = 20, col = "orange")

points(Close.Amount ~ Claim.Amount, data = dat[dat$Claim.Site == "Bus Station",], pch = 20, col = "yellowgreen")

sites = c("Bus Station", "Motor Vehicle", "Checkpoint", "Checked Baggage")
col.sites = c("yellowgreen", "orange", "mediumblue", "red4")

legend('topleft',legend = sites, col=col.sites, pch=19)

```

***

## Models

***

### Model 1 - Naive Bayes

We are using Naive Bayes here to make a prediction on whether or not a claim was approved or denied. Claims that have been settled have been changed to be approved in a separate column because the client has still received some money from it.

This first model made uses the Claim Amount, Airport Name, and Airline Name as the predictors. 

```{r}
# Split the data into training and test with 3:1 ratio
splits = split_data(dat, frac=c(3,1))
tr_dat = splits[[1]]
te_dat = splits[[2]]
# fitting model
fit = naiveBayes(DeniedApproved ~ Claim.Amount + Airport.Name + Airline.Name, data=tr_dat)
# compute confusion matrix
predicts = predict(fit, newdata=te_dat)
actuals = te_dat$DeniedApproved
conf_mtx = table(predicts, actuals)
conf_mtx
round(mean(predicts == actuals), 3)
```

The accuracy of the first model is around 54%.

The double density plot of this model is shown here.

```{r}
plot(density(as.numeric(predicts[actuals=="Approved"])), main="Double Density Plot", xlab="Approval Output", ylab="Density", ylim=c(0,20), col="red4", lwd=2)
lines(density(as.numeric(predicts[actuals=="denied"])), col="blue", lwd=2)
legend(1.4, 18, c("Approved", "Denied"), lty=c(1,1), lwd=c(2,2),col=c("red4","blue"))
```

```{r}
te_errs = c()
tr_errs = c()
te_actual = te_dat$DeniedApproved
tr_sizes = seq(100, nrow(tr_dat), length.out=20)
for (tr_size in tr_sizes) {
  tr_dat1 = tr_dat[1:tr_size,]
  tr_actual = tr_dat1$DeniedApproved
  fit = naiveBayes(DeniedApproved ~ Claim.Amount + Airport.Name + Airline.Name, data=tr_dat1)

  # error on training set
  tr_predicted = predict(fit, tr_dat1)
  err = mean(tr_actual != tr_predicted)
  tr_errs = c(tr_errs, err)

  # error on test set
  te_predicted = predict(fit, te_dat)
  err = mean(te_actual != te_predicted)
  te_errs = c(te_errs, err)
}
```

The learning curve here appears to show a very high bias situation.

```{r}
plot(tr_sizes, tr_errs, main="Learning Curve for Naive Bayes Approval/Denial - Model 1", xlab="Training Set Size", ylab="Error", ylim=c(0.15,0.5), col="red", type="b")
lines(tr_sizes, te_errs, col="blue", type="b")
legend(20000, 0.3, c("Training Set Error", "Test Set Error"), fill=c("red", "blue"))
```

### Model 2 - Naive Bayes

The second model for Naive Bayes uses the Airport Name and Airline Name again but this time, it switched the Claim Amount to be the Claim Type.

```{r}
# Fitting model
fit2 = naiveBayes(DeniedApproved ~ Airport.Name + Airline.Name + Claim.Type, data=tr_dat)
# compute confusion matrix
predicts = predict(fit2, newdata=te_dat)
actuals = te_dat$DeniedApproved
conf_mtx = table(predicts, actuals)
conf_mtx
round(mean(predicts == actuals), 3)
```

The accuracy of this second model is slightly better at 62%.

The double density plot of the second model is shown here.

```{r}
plot(density(as.numeric(predicts[actuals=="Approved"])), main="Double Density Plot", xlab="Approval Output", ylab="Density", ylim=c(0,10), col="red4", lwd=2)
lines(density(as.numeric(predicts[actuals=="denied"])), col="blue", lwd=2)
legend(0.9, 9, c("Approved", "Denied"), lty=c(1,1), lwd=c(2,2),col=c("red4","blue"))
```

```{r}
te_errs = c()
tr_errs = c()
te_actual = te_dat$DeniedApproved
tr_sizes = seq(100, nrow(tr_dat), length.out=20)
for (tr_size in tr_sizes) {
  tr_dat1 = tr_dat[1:tr_size,]
  tr_actual = tr_dat1$DeniedApproved
  fit2 = naiveBayes(DeniedApproved ~ Airport.Name + Airline.Name + Claim.Type, data=tr_dat1)

  # error on training set
  tr_predicted = predict(fit2, tr_dat1)
  err = mean(tr_actual != tr_predicted)
  tr_errs = c(tr_errs, err)

  # error on test set
  te_predicted = predict(fit2, te_dat)
  err = mean(te_actual != te_predicted)
  te_errs = c(te_errs, err)
}
```

The learning curve of the second model also appears to have a high bias but not as much as the first model.

```{r}
plot(tr_sizes, tr_errs, main="Learning Curve for Naive Bayes Approval/Denial - Model 2", xlab="Training Set Size", ylab="Error", ylim=c(0.15,0.5), col="red", type="b")
lines(tr_sizes, te_errs, col="blue", type="b")
legend(20000, 0.3, c("Training Set Error", "Test Set Error"), fill=c("red", "blue"))
```

### Model 3 - Naive Bayes

The third model for Naive Bayes will use both of the first model and second model. This means that the preidctors will be the Airport Name, Airline Name, Claim Type, and Claim Amount.

```{r}
# Fitting model
fit3 = naiveBayes(DeniedApproved ~ Airport.Name + Airline.Name + Claim.Type + Claim.Amount, data=tr_dat)
# compute confusion matrix
predicts = predict(fit3, newdata=te_dat)
actuals = te_dat$DeniedApproved
conf_mtx = table(predicts, actuals)
conf_mtx
round(mean(predicts == actuals), 3)
```

Even with using both models, it appears that the accuracy of the third model is lower than the accuracy of the second model but slightly higher than the first model. The third model is sitting at an accuracy of around 55%.

The double density plot of the third model is shown here.

```{r}
plot(density(as.numeric(predicts[actuals=="Approved"])), main="Double Density Plot", xlab="Approval Output", ylab="Density", ylim=c(0,10), col="red4", lwd=2)
lines(density(as.numeric(predicts[actuals=="denied"])), col="blue", lwd=2)
legend(0.9, 9, c("Approved", "Denied"), lty=c(1,1), lwd=c(2,2),col=c("red4","blue"))
```

```{r}
te_errs = c()
tr_errs = c()
te_actual = te_dat$DeniedApproved
tr_sizes = seq(100, nrow(tr_dat), length.out=20)
for (tr_size in tr_sizes) {
  tr_dat1 = tr_dat[1:tr_size,]
  tr_actual = tr_dat1$DeniedApproved
  fit3 = naiveBayes(DeniedApproved ~ Airport.Name + Airline.Name + Claim.Type + Claim.Amount, data=tr_dat1)

  # error on training set
  tr_predicted = predict(fit3, tr_dat1)
  err = mean(tr_actual != tr_predicted)
  tr_errs = c(tr_errs, err)

  # error on test set
  te_predicted = predict(fit2, te_dat)
  err = mean(te_actual != te_predicted)
  te_errs = c(te_errs, err)
}
```

It appears that the learning curve of the third model has a strangely high variance. The training set error also appearns to be higher than the test set error.

```{r}
plot(tr_sizes, tr_errs, main="Learning Curve for Naive Bayes Approval/Denial - Model 3", xlab="Training Set Size", ylab="Error", ylim=c(0.15,0.5), col="red", type="b")
lines(tr_sizes, te_errs, col="blue", type="b")
legend(20000, 0.3, c("Training Set Error", "Test Set Error"), fill=c("red", "blue"))
```

### Classification Tree Model

The following classification tree model is looking at the data `Claim.Site`, `Claim.Type`, and `Claim.Amount` to determine which combination of factor values will lead to a claim that is `Denied` or `Approved`. 

For this model, it is important to have two possible outcomes, but the data `Disposition` has three results: `Denied`, `Settled`, and `Approve in Full`. For the sake of this classification tree, a new column, `DeniedApproved`, has been created to consolidate `Settled` and `Approve in Full` values into the  `Approved` classification.

```{r}
splits = split_data(dat, frac=c(3,1))
tr_dat = splits[[1]]
te_dat = splits[[2]]

fit = rpart(DeniedApproved ~ Claim.Site + Claim.Type + Claim.Amount, dat = tr_dat, method = "class")
prp(fit, extra=106, varlen= 0,faclen=0, main = "", box.col = c("paleturquoise", "pink")[fit$frame$yval])
```


#### Classification Tree Assessment

The model created here has an accurace of: `65%`

Through this model we can see that if a claim amount is for less than $200, then it is more likely to be approved. 

If the claim type is a passenger theft, employee loss, or property damage and the incident occured at a bus station, checkpoint, or in motor vehicle, then it is also more likely to be approved.


It should be noted that the highest accuracy attained using various predictors is `68%`

This accuracy was attained using the predictors: `Claim.Site`, `Claim.Type`, `Claim.Amount`, `Airline.Name`, and `Item`.
There are many airlines that seem to be more likely to influence the approval of a claim. Unfortunately, the tree model would need to accomodate dozens of predictors and does not render well. Despite this, a learning curve testing this model is created and displayed below for model assessment.

Furthermore, using the `Airport.Code` did not alter the accuracy of the different models tested, so claim results are not dependent on the airport.

```{r}
# Test model:
predicted = predict(fit, te_dat, type = "class")
actual = te_dat$DeniedApproved
table(actual, predicted)
mean(actual == predicted)
```

#### Learning Curve: `Claim.Site`, `Claim.Type`,  `Claim.Amount`

```{r}
# Learning curve
te_errs = c()
tr_errs = c()
te_actual = te_dat$DeniedApproved
tr_sizes = seq(100, nrow(tr_dat), length.out=20)

for (tr_size in tr_sizes) {
  tr_dat1 = tr_dat[1:tr_size,]
  tr_actual = tr_dat1$DeniedApproved
  fit = rpart(DeniedApproved ~ Claim.Site + Claim.Type + Claim.Amount, dat = tr_dat1, method = "class")
  
  # error on training set
  tr_predicted = predict(fit, tr_dat1, type="class")
  err = mean(tr_actual != tr_predicted)
  tr_errs = c(tr_errs, err)

  # error on test set
  te_predicted = predict(fit, te_dat, type="class")
  err = mean(te_actual != te_predicted)
  te_errs = c(te_errs, err)
}

# Plot learning curve here
plot(tr_sizes, tr_errs , type = "b", ylim = c(0, 0.5), col = "red4", xlab = "Training Set Size", ylab = "Classification Error", main = "Learning Curve")
lines(tr_sizes, te_errs, type = "b", col = "blue")
legend("bottomright", c("Test Set Error", "Training Set Error"), pch = 1, col = c("blue", "red4"))
```

This learning curve above further assesses the Classification Tree Model displayed earlier in the project. We can see that the error rate for the test data decreases quickly, then levels off with increased number of data points. The training data error rate increases quickly, then levels to about the same classification error level as the test data error rate. This model is at risk for exhibiting higher bias as the predictors used may generate too general of a model to give accurate predictions.

The learning curve for a more accurate model will also be displayed below.

***

#### Learning Curve: `Claim.Site`, `Claim.Type`,  `Claim.Amount`, `Airline.Name`, `Item`

```{r}
# Learning curve
te_errs = c()
tr_errs = c()
te_actual = te_dat$DeniedApproved
tr_sizes = seq(100, nrow(tr_dat), length.out=20)

for (tr_size in tr_sizes) {
  tr_dat1 = tr_dat[1:tr_size,]
  tr_actual = tr_dat1$DeniedApproved
  fit = rpart(DeniedApproved ~ Claim.Site + Claim.Type + Claim.Amount + Airline.Name + Item, dat = tr_dat1, method = "class")
  
  # error on training set
  tr_predicted = predict(fit, tr_dat1, type="class")
  err = mean(tr_actual != tr_predicted)
  tr_errs = c(tr_errs, err)

  # error on test set
  te_predicted = predict(fit, te_dat, type="class")
  err = mean(te_actual != te_predicted)
  te_errs = c(te_errs, err)
}

# Plot learning curve here
plot(tr_sizes, tr_errs , type = "b", ylim = c(0, 0.5), col = "red4", xlab = "Training Set Size", ylab = "Classification Error", main = "Learning Curve")
lines(tr_sizes, te_errs, type = "b", col = "blue")
legend("bottomright", c("Test Set Error", "Training Set Error"), pch = 1, col = c("blue", "red4"))
```

This slightly more accurate model creates a learning curve with a noticeably lower classification error than seen for the previous model. Like the previous learning curve, we can see that the error rate for the test data decreases quickly while the error rate for the training data quickly increases. Both error rates for the training data and test data level off, but with a slight increase as the training set size increases. This model still exhibits some bias, but maybe not as much as the previous mode. The small gap between the error rates of the training data and test data indicate that more data could potentially increase the performance of this model and create more accurate predictions. Ultimately, it can be concluded that the airline associated with a claim and the item in question has more influence over whether a claim is likely to be denied or not.

***

## Conclusions

```{r}
#test stuff:
dat[dat$Claim.Amount == 0, c("Disposition", "Close.Amount", "Claim.Amount")]
```
